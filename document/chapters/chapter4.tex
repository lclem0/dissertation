\chapter{Mixed Reality for Human-Robot Collaboration}%
\label{chapter:on-site}
% \chapter{On-site Member's Application}% first title of section

After having properly implemented the bidirectional communication between both environments, the on-site and the remote, allowing both members to interact with the robot, the next step was to complement the already built application as well as its interface. 

\section{On-site Application Features}
\label{section:on-site-features}
% \input{chapters/on-site/on-site-features} commented this part because text is below - choose whether to use this or the text below
Regarding the on-site member's collaboration experience, as explained in the state of art review, by implementing different sensorial cues, such as visual and audio, it will enhance the user experience into a more intuitive and immersive way.
    
    \subsection{Virtual Safety Zones}
    \label{subsection:virtual-safety-zones} 
    % \input{chapters/on-site/subsections/virtual-safety-zones} commented this part because text is below - choose whether to use this or the text below

        The implementation of virtual safety zones is a critical feature designed to enhance on-site member's safety when interacting with the robot. 

        Two safety zones were developed, as shown in figure \ref{fig:dual-safety}, to address specific safety and user experience concerns:

        \begin{itemize}
            \item \textbf{Outer Safety Zone}: Initially, only the outer safety zone was developed. The purpose of creating this zone was to provide an 
            early warning to users as they approach the hazardous area near the robot. This approach consisted on changing its color as a visual alert. 
            However, this method proved ineffective because, once inside it, users could not perceive the color change, rendering the warning system inadequate.
            
            \item \textbf{Inner Safety Zone}: To overcome the limitations of the outer zone, an additional, inner safety zone was introduced. 
            This design ensures a two-step safety mechanism:

                \textbf{Visual Alert}: Upon entering the outer safety zone, the color of the inner sphere changes to red. 
                This alteration serves as a visual cue, indicating that the user is getting closer to a high-risk area.

                \textbf{Auditory Warning}: Entering the inner safety zone triggers an auditory alarm. This sound alert signifies that the user has 
                breached into the robot working area, enhancing the effectiveness of the safety mechanism.
        \end{itemize}

        \begin{figure}[h]
            \centering
            \includegraphics[width=0.5\linewidth]{figs/dual-safetyzone-robot.png}
            \caption{Simulated environment showing the display of both the inner and outer safety zones, as well as the robot and the marker inside them}
            \label{fig:dual-safety}
        \end{figure}

        \subsubsection{Safety Zone Breach Protocol}

            A crucial safety feature is that if the on-site member enters the safety zone area while the robot is in motion, the robot 
            automatically stops. This immediate halt ensures that potential accidents or injuries are avoided by preventing any interaction 
            with the robot when a user is within a designated dangerous area.
                
    \subsection{Interface}

        The interface, which incorporates the safety-zone features detailed earlier, is illustrated in figure \ref{} (take another picture with the most recent interface) 
        Within this interface, one can observe the panel for controlling the robot joints. Additionally, there are two green buttons 
        positioned at the top and bottom right corners of the interface, designated for activating the safety-zone and joint movement functions, 
        respectively. This subsequent features will be explained below.

        \begin{figure}[h]
            \centering
            \includegraphics[width=1\linewidth]{figs/interface.png}
            \caption{Interface featuring developed interactions}
            \label{fig: interface}
        \end{figure}

        \subsubsection{Joint Movement} - this is explained also in the previous chapter (3) - includes a pseudo-code snippet

            This feature allows users to manipulate the robot's joints via a user-friendly interface, as depicted in the figure \ref{fig:joint-1} 
            by the first joint. Its functionality works as follows:


            \begin{itemize}
                \item \textbf{Joint Selection}: Users can activate a joint by clicking on it. Upon activation, the 
                selected joint's central red circle turns green.
                \item \textbf{Movement Control}: By clicking on the selected joint's directional arrows within the interface, it moves in either a 
                positive or negative direction. This functionality mimics the real-time movement control similar to using keyboard arrow keys, ensuring 
                intuitive operation.
                \item \textbf{Continuous Movement}: The selected joint continues to move until we deactivate its button.
                \item \textbf{Single Joint Activation}: To ensure precise control, joint movement is only possible when one joint is selected at a time. 
                This prevents unintended actions and enhances the accuracy of adjustments.
            \end{itemize}
        
        
        \subsubsection{Buttons}
        
            Two buttons, shown in figure \ref{fig:joint-toggle}, were introduced to toggle the activation and deactivation of the joint menu 
            functionalities \ref{fig:toggle-joint} and the safety zones \ref{fig:toggle-safety}. Upon deactivation, the button will turn gray. 
            This design allows users to easily switch these features on or off, providing flexibility in controlling the safety mechanisms and movement 
            operations within the \ac{XR} environment.
            
            
            \begin{figure}[h]
            \centering
            \begin{subfigure}[b]{0.31\textwidth}    
                \centering
                \includegraphics[width=1\linewidth]{figs/joint-1.png}
                \caption{Joint Control for first Joint}
                \label{fig:joint-1}
                \end{subfigure}
            \hfill % This command adds space between the subfigures
            \begin{subfigure}[b]{0.31\textwidth}
                \centering
                \includegraphics[width=0.3\linewidth]{figs/clicked_joints.png}
                \caption{Controller Menu Toggle}
                \label{fig:toggle-joint}
            \end{subfigure}
            \hfill
            \begin{subfigure}[b]{0.31\textwidth}
                \centering
                \includegraphics[width=0.3\linewidth]{figs/unclick_sz.png}
                \caption{Safety-zone Toggle}
                \label{fig:toggle-safety}
            \end{subfigure}
            \caption{Example from first joint control menu and toggle buttons for controller menu and safety-zones}
            \label{fig:joint-toggle}
            \end{figure}
        
% \section{Video Demo}
%     The video demonstration (\href{https://www.youtube.com/watch?v=SMQ0yXhdnUo}{https://www.youtube.com/watch?v=SMQ0yXhdnUo}) showcases 
%     the successful implementation of the previously described features. Testing of the application was conducted using a laptop paired
%      with the camera depicted in the figure \ref{fig:camera-c922}. This approach was chosen due to encountered challenges in properly
%       building the application for android \ac{HHD}.
            

% previously was a chapter - re-define its order in the document
\section{Remote Collaboration}
% \label{chapter:remote}
% \begin{introduction}
% The development of the remote member's application will be thoroughly explained in this chapter.
% \end{introduction}

    After having successfully implemented the robot's digital model into the Unity environment, as well as performed the pose registration, 



    Building on the foundation of the application that facilitated on-site member interactions, the next critical step was to establish a robust connection to the UR10e robot. This connection was essential for accurately developing a digital twin of the robotic arm, allowing for real-time manipulation and visualization within a Unity environment.

    To achieve this, it was necessary to extend the initial project by developing a complementary application focused on the remote manipulation 
    of the robotic arm through Unity. 

    This bridge facilitated communication between the ROS environment on Ubuntu and the Unity application on Windows, enabling real-time visualization 
    and control of the robot within Unity.

    However, this integration posed significant challenges. Despite the potential of the ROS-TCP-Connector, the documentation provided limited guidance 
    on adapting the connection to different robots beyond the examples in the online tutorial. As a result, the development process relied heavily on 
    trial and error, requiring iterative testing and debugging to achieve a functional ROS-Unity connection.

    

    \section{Robot Manipulation}
When it it came to manipulate the digital version of the robot in the Unity environment, it was necessary to understand the Unity Robotics Hub package's way of doing so. A C\# script named \texttt{Controller.cs}, contained the necessary functions to control the robot's joints.


After further analysis, three control methods were implemented to control the robot's joints:
\begin{itemize}
    \item \textbf{UI Control:} This method allowed the user to control the \ac{DT} version of the robot by moving each joint individually through an Unity \ac{UI}. Its purpose consisted on being user-friendly and intuitive manner of controlling the robot, where a panel with a button for each joint was displayed, as shown in figure \ref{f:ui-control}. 
    
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.2\textwidth]{figs/toggle-1.png}
        \caption{\ac{UI} panel to control the robot's joints individually when using the \ac{UI} Control method}
        \label{f:ui-control}
    \end{figure}
    \FloatBarrier

    Apart from the joints' buttons, there is also a figure of the robot displayed in the top part of the panel, showing each joint's relative number, and thus allowing an easier identification of the joint to be controlled.
    
    Upon activating a joint by pressing the desired button, its color turns green instead of the default red button and the user is able to choose between rotating this joint in either the positive or negative direction, also represented by a change in the default color of the corresponding directional arrow. These features are represented in figure --- add figures joint being selected as well as the direction of UI control.
    
   
    \item \textbf{Unity-ROS Control:} This method enabled the robot to be controled during the runtime simulation in the Unity environment via the keyboard arrows of the laptop, then sending its position into the ROS environment via Wi-fi. 
    In order to change the Unity \ac{DT} robot state, the user has to press the right/left arrow keyboard keys to select the following/previous joint as well as the up/down keys to rotate the selected joint in the positive/negative direction, respectively.
    
    After updating the \ac{DT} robot's state, the user must press the "Publish" button within the user interface, shown in figure \ref{fig:publish_UI_button}. This action publishes the current joint states over Wi-Fi to the ROS environment in a different \ac{ROS} topic than the real robot joint states are defined. 

    \begin{figure}[htpb]
        \centering
        \includegraphics[width=0.9\linewidth]{figs/publish.jpeg}
        \caption{Publish button that sends Unity's \ac{DT} robot joint states into ROS the environment}
        \label{fig:publish_UI_button}
    \end{figure}
    
    Below, a pseudo-code showcasing how to manipulate the robot in Unity-ROS control mode is described.
    \begin{algorithm}
        \caption{Unity Input for Joint Selection and Movement}\label{alg:unity_input}
        \begin{algorithmic}[1]
            \State \textbf{Step 1: User Input for Joint Selection and Movement in Unity}
            \While{Unity Simulation is running AND Unity-ROS Control is selected}
                \If{RightArrowKeyPressed}
                    \State Select next joint
                \ElsIf{LeftArrowKeyPressed}
                    \State Select previous joint
                \EndIf
                \If{UpArrowKeyPressed}
                    \State Rotate selected joint in positive direction
                \ElsIf{DownArrowKeyPressed}
                    \State Rotate selected joint in negative direction
                \EndIf
                \If{PublishButtonPressed}
                    \State joint\_states = GetCurrentJointStates()
                    \State PublishToROSTopic('unity\_joint\_states', joint\_states)
                \EndIf
            \EndWhile
        \end{algorithmic}
    \end{algorithm}

    In order to handle this communication process, two ROS nodes were created. 
    The \texttt{unity\_joint\_subscriber.py} script was developed within the \texttt{iris\_ur10e} package.
    By creating a new node, called \texttt{joint\_state\_listener}, it then subscribes to the \texttt{unity\_joint\_states} topic, expecting to receive a JointState message type.
    Afterwards, it initializes a publisher for the \texttt{move\_joint\_unity} topic, that converts this data into a Float64MultiArray format that will be further received by the second node.
    
    Regarding the second node, the \texttt{move\_unity.py} script was created in the \texttt{iris\_sami} package. It initializes the \texttt{test\_arm\_movement} node that listens for joint position commands on the \texttt{/move\_joint\_unity} topic.
    Upon receiving this data, it moves the robotic arm to the desired position.

    This robot position update can be performed either on the simulation environment, where it can be visualized through Rviz, or by utilizing the real UR10e robot. 

    A pseudo-code explanation for the ROS nodes is presented in algorithm \ref{alg:combined_ros_node}.
    (add a picture of the framework exchange between Unity and ROS and a pseudo code of the ROS nodes created)


    % Part 2: ROS Node for Receiving Unity Joint States

    \begin{algorithm}
        \caption{Combined ROS Node for Receiving Unity Joint States and Moving the Robot}\label{alg:combined_ros_node}
        \begin{algorithmic}[1]
            \State \textbf{Step 2 and 3: Combined ROS Node for Receiving Unity Joint States and Moving the Robot}
            
            % Initialize first node
            \State Initialize ROS Node: joint\_state\_listener
            \State Subscribe to Topic: 'unity\_joint\_states'
            
            \While{Receiving JointState message from Unity}
                \State float\_array\_data = ConvertToFloat64MultiArray(joint\_states)
                \State PublishToROSTopic('move\_joint\_unity', float\_array\_data)
            \EndWhile
            
            % Initialize second node dependent on the first node
            \State \textbf{Precondition:} The \texttt{joint\_state\_listener} node must be running and publishing to the \texttt{'move\_joint\_unity'} topic.
            \State Initialize ROS Node: test\_arm\_movement
            \State Subscribe to Topic: 'move\_joint\_unity'
            
            \While{Receiving Float64MultiArray message from \texttt{move\_joint\_unity}}
                \State MoveRobotArmTo(joint\_positions)
                \If{ConnectedToRealRobot}
                    \State MoveRealRobot()
                \Else
                    \State VisualizeInRviz()
                \EndIf
            \EndWhile
        \end{algorithmic}
    \end{algorithm}
    
        

    \item \textbf{ROS-Unity Control:} This method allowed the robot to be controlled either in simulation in ROS environment with the Rviz interface, or manually in the robot itself and this would be reflected on the \ac{DT} robot model displayed in the Unity environment.

    Upon selecting this method, the \ac{DT} robot model would be updated in real-time according to the data that is being published into the \texttt{joint\_states} ROS topic.
    
    A new script was created in Unity, called \texttt{JointStateSubscriber.cs}, that subscribes to the \texttt{joint\_states} topic, and stores the information regarding the joint positions in a dictionary structure that is saved at each frame into a specific \texttt{.json} file. This file is then read by the \texttt{Controller.cs} script, updating the \ac{DT} robot model in Unity.
    
    By maintaining this synchronization between the real robot and the virtual environment, the Unity scene accurately reflects the robot's live state, ensuring a consistent digital twin representation as well as enabling the bidirectional communication and robot manipulation.

    Below, there is another pseudo-code that explains how the ROS-Unity control method works.
    \begin{algorithm}
        \caption{ROS-Unity Control via Joint States Subscription}\label{alg:ros_unity_control}
        \begin{algorithmic}[1]
            \State \textbf{Step 1: Subscribe to ROS \texttt{joint\_states} topic}
            \State Attach the Unity Script to the Digital Robot Model Asset: \texttt{JointStateSubscriber.cs}
            \State Upon Initialization, it subscribes to topic: \texttt{/joint\_states}
    
            \While{Receiving JointState message from ROS}
                \State Extract joint names and positions from the message
                \State Store joint positions in a dictionary structure
                \State Save the joint positions to a \texttt{jointStateSubscriber.json} file
            \EndWhile
    
            \State \textbf{Step 2: Update Unity \ac{DT} Robot Model}
            \While{Simulation is Running}
                \State Read the \texttt{jointStateSubscriber.json} file
                \State Update the Unity \ac{DT} robot model using the joint positions from the file
            \EndWhile
    
            \State \textbf{Step 3: Synchronize Real Robot with \ac{DT} Robot}
            \State The Unity \ac{DT} robot model moves according to the real robot’s joint positions, ensuring a consistent Digital Twin representation.
        \end{algorithmic}
    \end{algorithm}
    
\end{itemize}

    
    % \section{New Control Types for Remote Operation}
    % To maintain previously developed code and introduce necessary functionalities for remote operation, three distinct control types were implemented in the \texttt{Controller.cs} script, responsible for controlling the digital twin version of the robot:
    
    % \begin{enumerate}
    %     \item \textbf{UIButtonControl:} Originally developed for the on-site member application, this control type features a UI interface with safety-zone functionalities. It allows operators to interact safely and efficiently with the robot. Its implementation was detailed in the previous chapter.
    %     \item \textbf{Position Control:} This control type enables the manual control of the robot's joints through direct user input. It captures and sends the Unity digital twin's joint positions to the ROS environment upon user command, facilitating precise adjustments and real-time interaction.
    %     \item \textbf{Joint State Subscription:} This control type continuously updates the Unity digital twin based on joint state data received from ROS. It ensures the digital twin accurately reflects the physical robot's status, automating synchronization between the Unity model and the ROS data.
    % \end{enumerate}
    
 
    
    \subsubsection{Integration Highlights}
    These two nodes addressed key aspects of system performance:
    \begin{itemize}
        \item \textbf{Synchronization:} Ensures that changes in Unity’s control environment are accurately and timely reflected in the robot's physical movements.
        \item \textbf{Modularity:} Separates data handling and robot control into different nodes to improve system reliability and ease of maintenance.
    \end{itemize}




    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    do below here - add photos or video that mimic the process, same images
    
    \section{Camera Feed Transmission}

    In order to enhance the remote participant's understanding of the on-site environment and task performance, I integrated a live camera feed from a camera that was attached to the robot. This feed was then displayed in the Unity application, allowing the remote user to observe the robot's environment in real-time, providing critical visual feedback necessary for effective remote collaboration.

    \subsection{Hardware and Software Setup}
    An Orbbec Astra camera, displayed in the figure \ref{fig:astra-camera}, was provided by the project supervisors was used to capture the live video feed. This 3D camera was chosen for its high-quality video output and compatibility with the ROS environment, enabling seamless integration into the Unity application.
    
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.7\textwidth]{figs/AstraSeries_3.jpg}
        \caption{Astra 3D Orbbec Camera used to transmit real-time video feed from robot environment to remote user}
        \label{fig:astra-camera}
    \end{figure}
    \FloatBarrier

    To integrate the camera into the ROS environment, an existing GitHub repository \footnote{Github Repository used to integrate Astra Orbbec Camera in the ROS environment \url{https://github.com/orbbec/ros_astra_camera} Accessed: 2024-10-04} tailored for the Astra camera integration was used. This repository contained the necessary drivers and ROS nodes to enable the camera's functionality within the ROS framework.

    \subsection{ROS Camera Node}
    To initiate the camera feed, the \texttt{astra\_camera\_node} from the \texttt{astra\_camera} package is initialized. Afterwards, an RVIZ image viewer was used to visualize the live feed, ensuring the camera was functioning correctly and capturing the desired video data.

    \subsection{Unity Camera Feed Integration}
    From the Unity side, the \texttt{CameraFeedReceiver.cs} (verify the name of the script) script was developed to receive and display the live camera feed. This script was then atached to an UI interface (confirm the name of the element) that displayed the video feed in real-time. 
    add a figure of the UI interface with the view of the camera - The figure (add figure) illustrates the camera feed in the Unity application, showcasing the live video stream from the robot's environment.

    \subsection{Data Transmission to Unity} 
     
    However, the raw image data generated by the camera was too heavy to be transmitted efficiently over Wi-Fi, so this data needed to be republished using the \texttt{image\_transport} package.
    By utilizing the following \ac{ROS} command 
    \begin{verbatim}
        rosrun image_transport republish raw 
        in:=/camera/color/image_raw out:=/camera/image_repub
    \end{verbatim}
    the image data was republished in a more efficient format, allowing for smoother and real-time transmission to the Unity environment.
    


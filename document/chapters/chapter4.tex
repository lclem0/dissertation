\chapter{Mixed Reality for Human-Robot Collaboration}%
\label{chapter:on-site}
% \chapter{On-site Member's Application}% first title of section


\section{Framework}

Figure~\ref{fig:project_framework} illustrates the proposed framework of the \ac{MR}-based \ac{HRC} system. This framework seamlessly integrates two distinct environments—on-site and remote—through a robust communication pipeline designed to enable real-time, collaborative robot manipulation.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/framework-1.jpeg}
    \caption{Overview of the proposed \ac{MR}-based \ac{HRC} system framework integrating remote and on-site environments.}
    \label{fig:project_framework}
\end{figure}
* TODO: verify when on-site uses member and verify if it should be changed to another name (user, counterpart - verify with Bernardo's notes)
* TODO: add to the figure the camera display and the camera in the on-site environment 
First of all, regarding the \textbf{On-Site Environment}, the UR10e robotic arm operates as the central physical entity to be controlled and manipulated. The on-site member interacts with this robotic arm through a custom \ac{MR} application developed in Unity, chosen due to its robust \ac{MR} capabilities, which facilitate the creation of immersive, interactive environments, enabling intuitive real-time robot manipulation.

After implementing the robot digital model into the simulation environment, the next step was to align it with the physical robot. Pose registration between the physical robot and its digital counterpart is executed using Vuforia's capabilities. The system employs ArUco markers for precise pose estimation, ensuring that the digital representation of the UR10e is accurately aligned with its physical counterpart. The \ac{DT} of the robot, rendered in Unity, provides a visually synchronized, real-time mirror of the robot's movements and configurations, thus facilitating enhanced interaction.

The robot is connected via Ethernet to a laptop running Ubuntu 20.04 with \ac{ROS} Noetic, which serves as the middleware layer. This setup facilitates seamless data exchange between the Unity \ac{DT} and the physical robot. The \texttt{iris\_ur10e} and \texttt{iris\_sami} \ac{ROS} packages, developed by the IRIS Lab and available on GitHub\footnote{https://github.com/iris-ua}, provide a pre-established \ac{ROS} environment that supports critical functionalities such as trajectory planning and robotic manipulation and visualization through RViz.

To tailor these packages to the specific needs of this project, several enhancements were made. These modifications included the integration of bidirectional data flow between \ac{ROS} and Unity, enabling the Unity-based \ac{DT} to mirror the real-time movements of the physical robot. In particular, new \ac{ROS} nodes were created to subscribe to joint state data from the physical robot and publish these updates to Unity, ensuring precise synchronization between the physical and virtual environments. Additionally, new publishing mechanisms were implemented to send commands from Unity back to \ac{ROS}, allowing for full control of the robot through the \ac{MR} interface. These modifications were essential for realizing the bidirectional communication required for accurate \ac{DT} manipulation.

This communication between the \ac{ROS} middleware and Unity’s \ac{MR} environment is established using Unity’s \ac{ROS}-\ac{TCP}-Connector and \ac{ROS}-\ac{TCP}-Endpoint packages. These packages enable bidirectional communication over a \ac{TCP}/\ac{IP} protocol, ensuring real-time synchronization between the virtual and physical environments. This communication architecture is fundamental for maintaining the \ac{DT}'s fidelity, reflecting real-world changes in the Unity model and vice versa, as referred in the section \ref{sec:dt}.

Regarding the \textbf{Remote Environment}, a remote participant accesses the same Unity-\ac{MR} application. This allows the remote member to visualize and manipulate the robot in real time, from a separate location. The remote \ac{UI} provides real-time visualization of the robot’s state and its workspace, enabling remote collaboration. The synchronization between the remote and on-site environments is facilitated through Unity’s \ac{MR} capabilities, which, in conjunction with the \ac{ROS}-based control, enable the remote user to execute commands and receive real-time feedback.

The Middleware layer, acting as the system’s backbone, ensures the continuous synchronization of data between the physical robot and its \ac{DT}. It manages the real-time feedback loop, maintaining bidirectional data flow between the virtual robot in Unity and the physical robot in the on-site environment. This configuration guarantees that any actions performed by either the on-site or remote user are consistently reflected in both the physical and digital realms, preserving operational coherence and maximizing collaborative efficiency.

This framework provides an immersive and responsive \ac{MR} environment, bridging the gap between physical and digital spaces. The system enables real-time robot manipulation and monitoring from both on-site and remote locations, making it a versatile platform for collaborative tasks in advanced industrial applications. The seamless integration of \ac{MR}, \ac{DT}, and \ac{HRC} technologies significantly enhances user interaction, safety, and productivity, while offering an intuitive interface for remote and on-site collaboration.


*TODO: see where to start explaining what was done regarding the control methods.
Three distinct control methods were developed within the application to facilitate manipulation, which will be explained further below.


\section{Mixed Reality Environment}

Regarding the Unity developed \ac{MR} environment, the development started by implementing the proper UR10e digital model. This model was then aligned with the physical robot through Vuforia's pose registration capabilities, ensuring that later the \ac{DT} would accurately mirror the robot movements and configurations.

The base robot manipulation provided by the \ac{URDF}-Importer package consisted on moving each joint individually by selecting the desired joint and then moving it with by using the keyboard directional arrows. This method was improved to three distinct control methods that enhance user experience and facilitate robot manipulation.

\section{Robot Manipulation}
In order to properly develop new ways of controlling the \ac{DT} version of the robot, it was necessary to understand the C\# script named \texttt{Controller.cs}, which contained all the necessary functions to control the robot's joints.


After further analysis, three control methods were implemented to control the robot's joints:
% \begin{itemize}
    *TODO: add figures of: joint button selected and not selected (red and green)
    *TODO: add figures of: joint direction selected and not selected (white and green)
    * TODO: confirm if it is not possible to move the robot using other methods whenever the \ac{UI} interface is active. 
    * TODO: add a figure of the toggle button activated and deactivated (green and gray) as well as the impact it has on the menu - showing it activated and deactivated (take a screenshot of the whole interface with the menu activated and deactivated)

    \textbf{UI Control:} This method allowed the user to control the \ac{DT} version of the robot by moving each joint individually through an Unity \ac{UI}. Its purpose consisted on being user-friendly and an intuitive manner of controlling the robot. Apart from the joints' buttons, there is also a figure of the robot displayed in the top part of the panel, showing each joint's relative number, and thus allowing an easier identification of the joint to be controlled. This menu is displayed in the Figure~\ref{fig: interface}.
    
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.2\textwidth]{figs/toggle-1.png}
        \caption{\ac{UI} panel to control the robot's joints individually when using the \ac{UI} Control method}
        \label{f:ui-control}
    \end{figure}
    % \FloatBarrier

    % Upon pressing a joint's respective button, its color changes from the default red to green, allowing the user to understand which joint is currently selected. Besides, the user can also choose between rotating the selected joint in either the positive or negative direction, which is also represented by a change in the default color of the corresponding directional arrow. These features are represented in Figure: 

    In this control method, only a joint can be moved at a time, so whenever two joints are selected in the \ac{UI} menu, none of the remaining joints will be able to rotate.
    The developed features of the \ac{UI} control method are described below:
    \begin{itemize}
        \item \textbf{Joint Selection}: Users can activate a joint by clicking on it. Upon activation, the selected joint's central red circle turns green.
        \item \textbf{Movement Control}: By clicking on the selected joint's directional arrows within the interface, it moves in either a 
        positive or negative direction. This functionality mimics the real-time movement control similar to using keyboard arrow keys, ensuring 
        intuitive operation.
        \item \textbf{Continuous Movement}: The selected joint continues to move until we deactivate its respective button.
        \item \textbf{Single Joint Activation}: To ensure precise control, joint movement is only possible when one joint is selected at a time. 
        This prevents unintended actions and enhances the accuracy of adjustments.
    \end{itemize}
    
   
    % Additionally, there is a button that activates/deactivates the \ac{UI} menu. If the user wants more space to visualize the robot's movements, they can easily hide the \ac{UI} menu by pressing this button. This feature is represented in the Figure~\ref{fig:toggle-1} and it is also useful whenever the user wants to use other control methods, as it provides more space to visualize the robot's movements and moving the robot via the \ac{UI} interface is not possible anymore. 

    % this or the below commented version?
    In addition to the core controls, the \ac{UI} features a toggle button that allows the user to activate or deactivate the entire robot control panel. This is particularly useful when the user requires a larger viewport to observe the robot's movements or when switching between different control methods. When the \ac{UI} panel is hidden, the robot remains visible, but interaction via the \ac{UI} is disabled, freeing up screen space for other actions (Figure~\ref{fig: interface}). 
    % This toggle functionality also allows users to switch between different control methods while maintaining a clean and efficient workspace.

    % check which figure is this one and wheter to leave it or not?
    % \begin{figure}[h]
    % \centering
    % \includegraphics[width=1\linewidth]{figs/interface.png}
    % \caption{Interface featuring developed interactions}
    % \label{fig: interface}
    % \end{figure}


    \textbf{Unity-ROS Control:} This method enabled the robot to be controled during the runtime simulation in the Unity environment via the keyboard arrows. In order to change the \ac{DT}, the user has to press the right/left arrow keyboard keys to select the following/previous joint as well as the up/down keys to rotate the selected joint in the positive/negative direction, respectively.
         
    After updating the \ac{DT} robot's state, the user must press the "Publish" button within the \ac{UI}, shown in figure \ref{fig:publish_UI_button}. This action publishes the current \ac{DT} joint states using the \ac{ROS}-\ac{TCP}-Connector/Endpoint packages to the \ac{ROS} middleware, into a new \ac{ROS} topic. 

    \begin{figure}[htpb]
        \centering
        \includegraphics[width=0.9\linewidth]{figs/publish.jpeg}
        \caption{Publish button that sends Unity's \ac{DT} robot joint states into ROS the environment}
        \label{fig:publish_UI_button}
    \end{figure}
    
    Below, a pseudo-code showcasing how to manipulate the robot in Unity-ROS control mode is described.
    \begin{algorithm}
        \caption{Unity Input for Joint Selection and Movement}\label{alg:unity_input}
        \begin{algorithmic}[1]
            \State \textbf{Step 1: User Input for Joint Selection and Movement in Unity}
            \While{Unity Simulation is running AND Unity-ROS Control is selected}
                \If{RightArrowKeyPressed}
                    \State Select next joint
                \ElsIf{LeftArrowKeyPressed}
                    \State Select previous joint
                \EndIf
                \If{UpArrowKeyPressed}
                    \State Rotate selected joint in positive direction
                \ElsIf{DownArrowKeyPressed}
                    \State Rotate selected joint in negative direction
                \EndIf
                \If{PublishButtonPressed}
                    \State joint\_states = GetCurrentJointStates()
                    \State PublishToROSTopic('unity\_joint\_states', joint\_states)
                \EndIf
            \EndWhile
        \end{algorithmic}
    \end{algorithm}

    To avoid conflicts with the \ac{ROS} node responsible for controlling the real robot's joints, which publishes to the standard \texttt{joint\_states} topic, a separate \ac{ROS} topic \texttt{(unity\_joint\_states)} was created to handle the joint data coming from Unity. This ensures that data from Unity does not interfere with the real robot’s ongoing operations. When the "Publish" button is pressed, the Unity-defined joint states are sent to this new topic, and only when necessary are they relayed to the real robot for movement execution. 
    
    In order to handle this communication process, two new \texttt{python} scripts were created in order to start two new \ac{ROS} nodes, the \texttt{joint\_state\_listener} and the \texttt{move\_unity} nodes. The \texttt{unity\_joint\_subscriber.py} script was developed within the \texttt{iris\_ur10e} package, initializing the node that subscribes to the \texttt{unity\_joint\_states} topic, expecting to receive a JointState message type. It then initializes a publisher for the \texttt{move\_joint\_unity} topic, converting this data into a Float64MultiArray format that will be further received by the second node. Regarding this second node, \texttt{move\_unity}, created within the \texttt{iris\_sami} package, it subscribes the \texttt{move\_joint\_unity} topic  to joint position commands, moving the robotic arm to the desired position. This robot position update can be performed either on the simulation environment, where it can be visualized through Rviz, or by utilizing the real UR10e robot. 
    
    % The \texttt{unity\_joint\_subscriber.py} script was developed within the \texttt{iris\_ur10e} package.
    % By creating a new node, called \texttt{joint\_state\_listener}, it then subscribes to the \texttt{unity\_joint\_states} topic, expecting to receive a JointState message type.
    % Afterwards, it initializes a publisher for the \texttt{move\_joint\_unity} topic, that converts this data into a Float64MultiArray format that will be further received by the second node.
    
    % Regarding the second node, the \texttt{move\_unity.py} script was created in the . It initializes the \texttt{test\_arm\_movement} node that listens for joint position commands on the \texttt{/move\_joint\_unity} topic.
    % Upon receiving this data, it moves the robotic arm to the desired position.


    A pseudo-code explanation for the \ac{ROS} nodes is presented in algorithm \ref{alg:combined_ros_node}.
    * TODO: add a picture of the framework exchange between Unity and ROS 

    % Part 2: ROS Node for Receiving Unity Joint States

    \begin{algorithm}
        \caption{Combined ROS Node for Receiving Unity Joint States and Moving the Robot}\label{alg:combined_ros_node}
        \begin{algorithmic}[1]
            \State \textbf{Step 2 and 3: Combined ROS Node for Receiving Unity Joint States and Moving the Robot}
            
            % Initialize first node
            \State Initialize ROS Node: joint\_state\_listener
            \State Subscribe to Topic: 'unity\_joint\_states'
            
            \While{Receiving JointState message from Unity}
                \State float\_array\_data = ConvertToFloat64MultiArray(joint\_states)
                \State PublishToROSTopic('move\_joint\_unity', float\_array\_data)
            \EndWhile
            
            % Initialize second node dependent on the first node
            \State \textbf{Precondition:} The \texttt{joint\_state\_listener} node must be running and publishing to the \texttt{'move\_joint\_unity'} topic.
            \State Initialize ROS Node: test\_arm\_movement
            \State Subscribe to Topic: 'move\_joint\_unity'
            
            \While{Receiving Float64MultiArray message from \texttt{move\_joint\_unity}}
                \State MoveRobotArmTo(joint\_positions)
                \If{ConnectedToRealRobot}
                    \State MoveRealRobot()
                \Else
                    \State VisualizeInRviz()
                \EndIf
            \EndWhile
        \end{algorithmic}
    \end{algorithm}
    
        

    \textbf{\ac{ROS}-Unity Control:} This control method allowed the robot to be controlled firstly via Rviz simulation or manually in the robot itself, in the \ac{ROS} middleware environment. These changes would be applied on the \ac{DT} robot model displayed in the Unity \ac{MR} environment. Upon selecting this method, the \ac{DT} robot model would be updated in real-time according to the data that is being published into the \texttt{joint\_states} \ac{ROS} topic.
    
    In order to properly achieve this communication and data trasnfer, a new script was created in Unity, \texttt{JointStateSubscriber.cs}. This script subscribes to the \texttt{joint\_states} topic, and stores the information regarding the joint positions in a dictionary structure that is saved at each frame into a specific \texttt{.json} file within the \ac{MR} environment. This file is constantly being read by the \texttt{Controller.cs} script whenever the simulation is running, updating the \ac{DT} robot model accordingly.
    
    By maintaining this synchronization between the real robot and the virtual environment, the Unity scene accurately reflects the robot's live state, ensuring a consistent \ac{DT} representation through the bidirectional communication established between the \ac{MR} environment and the \ac{ROS} middleware.

    * TODO: add here a figure of the Unity-ROS control method, showing the robot being controlled in the Unity environment while the real robot is moving as well.

    Below, there is another pseudo-code explaining how the \ac{ROS}-Unity control method works.
    \begin{algorithm}
        \caption{ROS-Unity Control via Joint States Subscription}\label{alg:ros_unity_control}
        \begin{algorithmic}[1]
            \State \textbf{Step 1: Subscribe to ROS \texttt{joint\_states} topic}
            \State Attach the Unity Script to the Digital Robot Model Asset: \texttt{JointStateSubscriber.cs}
            \State Upon Initialization, it subscribes to topic: \texttt{/joint\_states}
    
            \While{Receiving JointState message from ROS}
                \State Extract joint names and positions from the message
                \State Store joint positions in a dictionary structure
                \State Save the joint positions to a \texttt{jointStateSubscriber.json} file
            \EndWhile
    
            \State \textbf{Step 2: Update Unity \ac{DT} Robot Model}
            \While{Simulation is Running}
                \State Read the \texttt{jointStateSubscriber.json} file
                \State Update the Unity \ac{DT} robot model using the joint positions from the file
            \EndWhile
    
            \State \textbf{Step 3: Synchronize Real Robot with \ac{DT} Robot}
            \State The Unity \ac{DT} robot model moves according to the real robot’s joint positions, ensuring a consistent Digital Twin representation.
        \end{algorithmic}
    \end{algorithm}
    
% \end{itemize}

 
\section{On-site Mixed-Reality Features}
\label{section:on-site-features}
% \input{chapters/on-site/on-site-features} commented this part because text is below - choose whether to use this or the text below
After having properly implemented the control methods, the next step was to develop features that would enhance the on-site user's experience when interacting with the robot. These features were designed to improve user safety, facilitate robot manipulation, and provide an intuitive interface for controlling the robot's joints. The following sections detail the key features developed to enhance the on-site counterpart's interaction with the robot in the \ac{MR} environment.

% Regarding the on-site member's collaboration experience, as explained in the state of art review, by implementing different sensorial cues, such as visual and audio, it will enhance the user experience into a more intuitive and immersive way.
    
\subsection{Virtual Safety Zones}
\label{subsection:virtual-safety-zones} 
% \input{chapters/on-site/subsections/virtual-safety-zones} commented this part because text is below - choose whether to use this or the text below

As explained in section \ref{sec:hrc-in-industry}, implementating different sensorial cues enhances the user experience into a more intuitive and immersive way. Therefore, visualizing the working zone of the robot is a critical feature designed to enhance on-site member's safety when interacting with the robot. In order to achieve this, two safety zones were developed, as shown in figure \ref{fig:dual-safety}, to address specific safety and user experience concerns: 

\begin{itemize}
\item \textbf{Outer Safety Zone}: Initially, only the outer safety zone was developed. The purpose of creating this zone was to provide an 
early warning to users as they approach the hazardous area near the robot. This approach consisted on changing its color as a visual alert. 
However, this method proved ineffective because, once inside it, users could not perceive the color change, rendering the warning system inadequate.

\item \textbf{Inner Safety Zone}: To overcome this outer zone limitation, an additional inner safety zone was integrated. This design ensures a two-step safety mechanism that properly alerts users when they are in close proximity to the robot.

    \textbf{Visual Alert}: Upon entering the outer safety zone, the color of the inner sphere changes to red. This change serves as a visual cue, indicating that the user is getting closer to a high-risk area.

    \textbf{Auditory Warning}: Entering the inner safety zone triggers an auditory alarm signifying that the user has breached into the robot working area, enhancing the effectiveness of the safety mechanism.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{figs/dual-safetyzone-robot.png}
\caption{Simulated environment showing the display of both the inner and outer safety zones, as well as the robot and the marker inside them}
\label{fig:dual-safety}
\end{figure}
* TODO: re-do this figure with the most recent version of the UI

\subsubsection{Safety Zone Breach Protocol}
* TODO: which one, the inside or the outside safety zone that stops the robot? - confirm this part
Still regarding the safety-zone feature, a crucial safety protocol was implemented to ensure user safety when interacting with the robot. If the user member enters the safety zone area while the robot is in motion, the robot automatically stops. This immediate halt ensures that potential accidents or injuries are avoided by preventing any interaction with the robot when a user is within a designated dangerous area.
    
% this interface section - the feature of controlling it using the UI must be put in the UI COntrol method above
%  the remaining part should be in this part - explaining the rest of the UI features
\subsection{Interface}

% The interface, which incorporates the safety-zone features detailed earlier, is illustrated in Figure~\ref{}.
* TODO: re-do this figure with the most recent version of the UI

Besides the above described panel for controlling the digital robot's joints and the toggle button to activate/deactivate it, the interface also features another green button, designated for activating/deactivating the safety-zone functions. Upon deactivation, the button will turn gray. This design allows users to easily switch these features on or off, providing flexibility in controlling the safety mechanisms and movement operations within the \ac{MR} environment.

\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.31\textwidth}    
    \centering
    \includegraphics[width=1\linewidth]{figs/joint-1.png}
    \caption{Joint Control for first Joint}
    \label{fig:joint-1}
    \end{subfigure}
\hfill % This command adds space between the subfigures
\begin{subfigure}[b]{0.31\textwidth}
    \centering
    \includegraphics[width=0.3\linewidth]{figs/clicked_joints.png}
    \caption{Controller Menu Toggle}
    \label{fig:toggle-joint}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.31\textwidth}
    \centering
    \includegraphics[width=0.3\linewidth]{figs/unclick_sz.png}
    \caption{Safety-zone Toggle}
    \label{fig:toggle-safety}
\end{subfigure}
\caption{Example from first joint control menu and toggle buttons for controller menu and safety-zones}
\label{fig:joint-toggle}
\end{figure}

% \section{Video Demo}
%     The video demonstration (\href{https://www.youtube.com/watch?v=SMQ0yXhdnUo}{https://www.youtube.com/watch?v=SMQ0yXhdnUo}) showcases 
%     the successful implementation of the previously described features. Testing of the application was conducted using a laptop paired
%      with the camera depicted in the figure \ref{fig:camera-c922}. This approach was chosen due to encountered challenges in properly
%       building the application for android \ac{HHD}.


% continue here
\section{Camera Feed Transmission}

In order to enhance the remote participant's understanding of the on-site environment and task performance, another camera was added, allowing for feed transmission. 

\subsection{Hardware and Software Setup}
An Orbbec Astra camera~\footnote{\url{https://www.orbbec.com/products/structured-light-camera/astra-series/} Acessed:2024-10-22}, displayed in the figure \ref{fig:astra-camera}, was provided by the project supervisors was used to capture the live video feed. This 3D camera was chosen for its high-quality video output and compatibility with the \ac{ROS} environment.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/AstraSeries_3.jpg}
    \caption{Astra 3D Orbbec Camera used to transmit real-time video feed from robot environment to remote user}
    \label{fig:astra-camera}
\end{figure}
\FloatBarrier

To integrate the camera into the \ac{ROS} environment, an existing GitHub repository \footnote{\url{https://github.com/orbbec/ros_astra_camera} Accessed: 2024-10-04} tailored for the Astra camera integration was used. This repository contained the necessary drivers and \ac{ROS} nodes to enable the camera's functionality within the middleware framework.


\subsection{ROS Camera Node and Camera Transmission}

To enable the camera feed within the \ac{ROS} environment, the \texttt{astra\_camera\_node} from the \texttt{astra\_camera} package is initialized. This node captures live video data from the camera and displays it within the RViz interface to ensure the camera’s functionality and the accuracy of the captured data. This data needs to be transmitted to the Unity \ac{MR} application, allowing the remote user to observe the robot's environment in real-time, providing critical visual feedback necessary for effective remote collaboration.

However, transmitting the raw image data over the \ac{TCP} Connector/Endpoint package that enables the connection between both environments presents significant challenges due to its size and bandwidth requirements, which could affect real-time collaboration.
To overcome this, the \texttt{image\_transport} package is used. This package allows for the conversion of the raw image stream into a compressed format, reducing the data size without compromising significant image quality. In order to republish the image in a compressed format, the following \ac{ROS} command is used:

\begin{verbatim}
     rosrun image_transport republish raw 
     in:=/camera/color/image_raw out:=/camera/image_repub 
\end{verbatim}

By compressing the image data, smoother and more efficient real-time transmission to the Unity environment was achieved, ensuring minimal delay and consistent visual feedback for remote collaborators.

\subsection{Unity Camera Feed Integration}

After having compressed the image data, the next step was to integrate it into the Unity \ac{MR} environment. To facilitate this, a new script, \texttt{CameraFeedReceiver.cs} was developed, which handled the reception of the video feed and its subsequent rendering within the Unity application. The script was then attached to a designated \ac{UI} panel, ensuring the live camera feed could be displayed in real time for the remote users. 

*TODO: add a figure of the camera transmission in unity and rviz. should i say that it provided situational awareness of robot's surroundings, since it was not atached to the robot itself. 
The live feed provided critical situational awareness of the robot’s surroundings, aiding the remote operator in better understanding and interacting with the environment. Figure \ref{} illustrates this setup, showing the live camera feed within the Unity interface.




% To initiate the camera feed, the \texttt{astra\_camera\_node} from the \texttt{astra\_camera} package is initialized. Afterwards, an RVIZ image viewer displays the live feed, ensuring the camera is functioning correctly and capturing the desired video data.

% * TODO: add a picture of the UI showing that the camera live feed from Rviz
% * TODO: need to explain better how the image transmission is sent to the Unity, needed to compress the image data into a different format, from raw to compressed, to be able to send it over Wi-Fi - used the image\_transport package to republish the image data in a more efficient format, allowing for smoother and real-time transmission to the Unity environment.
% after launching the camera node, the image data was republished using the image\_transport package, which allowed for smoother and real-time transmission to the Unity environment.
% * TODO: reorganize this part better

% \subsection{Unity Camera Feed Integration}

% After the camera live feed was displayed in the Rviz simulation within the on-site environment, this data needs to be transmitted to the Unity \ac{MR} application, allowing the remote user to observe the robot's environment in real-time, providing critical visual feedback necessary for effective remote collaboration.


% From the Unity side, the \texttt{CameraFeedReceiver.cs} (verify the name of the script) script was developed to receive and display the live camera feed. This script was then atached to an UI interface (confirm the name of the element) that displayed the video feed in real-time. 
% add a figure of the UI interface with the view of the camera - The figure (add figure) illustrates the camera feed in the Unity application, showcasing the live video stream from the robot's environment.

% \subsection{Data Transmission to Unity} 
    
% However, the raw image data generated by the camera was too heavy to be transmitted efficiently over Wi-Fi, so this data needed to be republished using the \texttt{image\_transport} package.
% By utilizing the following \ac{ROS} command 
% \begin{verbatim}
%     rosrun image_transport republish raw 
%     in:=/camera/color/image_raw out:=/camera/image_repub
% \end{verbatim}
% the image data was republished in a more efficient format, allowing for smoother and real-time transmission to the Unity environment.


